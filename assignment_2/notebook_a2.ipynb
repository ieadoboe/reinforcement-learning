{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a52aa7d",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b9748",
   "metadata": {},
   "source": [
    "## Part 1 - Why Different Methods Converge to the Same Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c26fa64",
   "metadata": {},
   "source": [
    "### Mathematical Guarantee: Contraction Mapping Theorem\n",
    "\n",
    "The Bellman operator $T$ is a **contraction mapping**:\n",
    "$$||T V_1 - T V_2||_\\infty \\leq \\gamma ||V_1 - V_2||_\\infty$$\n",
    "\n",
    "Since $\\gamma < 1$, this guarantees:\n",
    "1. **Unique fixed point**: There exists exactly one $V^*$ such that $TV^* = V^*$\n",
    "2. **Convergence**: Any iterative method applying $T$ converges to $V^*$ regardless of initialization\n",
    "\n",
    "### Why the Same Optimal Policy?\n",
    "\n",
    "What's unique is $V^*$, not necessarily $\\pi^*$. However:\n",
    "- Any policy that acts greedily w.r.t. $V^*$ is optimal\n",
    "- All methods find the same $V^*$ (by uniqueness of fixed point)\n",
    "- Therefore, greedy policies derived from $V^*$ are equivalent in performance\n",
    "\n",
    "### Assumptions Required for Convergence\n",
    "\n",
    "1. **Finite state/action spaces** (satisfied in gridworld)\n",
    "2. **$\\gamma < 1$** (discount factor creates contraction)\n",
    "3. **Well-defined transition probabilities** (stochastic matrix properties)\n",
    "4. **Bounded rewards** (ensures finite value functions)\n",
    "\n",
    "**Note**: In gridworld with $\\gamma = 0.95$, all conditions are satisfied, guaranteeing convergence to the same optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa7dd1c",
   "metadata": {},
   "source": [
    "Under certain conditions, all these methods converge to the same optimal policy because there exists a unique optimal value function V* that satisfies the Bellman optimality equation:\n",
    "\n",
    "$$V^*(s) = max_a [R(s,a) + \\gamma\\sum P(s'|s,a) V^*(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d029a591",
   "metadata": {},
   "source": [
    "Explicit solution: Using GPS coordinates to jump directly to the peak\n",
    "Value iteration: Climbing uphill step by step, always choosing the steepest ascent\n",
    "Policy iteration: Alternating between exploring the current altitude everywhere (policy evaluation) and then moving to higher ground (policy improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f185b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import jit, lax\n",
    "\n",
    "@jit\n",
    "def bellman_update(V, R, P, gamma):\n",
    "    return R + gamma * jnp.dot(P, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc66e26",
   "metadata": {},
   "source": [
    "### Method 1: Explicit Bellman Solution\n",
    "\n",
    "For a fixed policy $\\pi$, the value function satisfies:\n",
    "$$V^\\pi = R^\\pi + \\gamma P^\\pi V^\\pi$$\n",
    "$$\\implies (I - \\gamma P^\\pi)V^\\pi = R^\\pi$$\n",
    "\n",
    "**Key insight**: This gives the exact solution in one step (assuming matrix invertibility).\n",
    "\n",
    "For the optimal policy, we solve:\n",
    "$$V^* = \\max_\\pi V^\\pi$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0545af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gridworld dimensions\n",
    "GRID_SIZE = 5\n",
    "N_STATES = GRID_SIZE * GRID_SIZE\n",
    "N_ACTIONS = 4  # up, down, left, right\n",
    "\n",
    "# Special states (convert to 1D indices)\n",
    "def coord_to_idx(row, col):\n",
    "    return row * GRID_SIZE + col\n",
    "\n",
    "BLUE_STATE = coord_to_idx(0, 1)    # (0,1) -> jumps to red, reward +5\n",
    "GREEN_STATE = coord_to_idx(0, 4)   # (0,4) -> jumps to yellow/red, reward +2.5  \n",
    "RED_STATE = coord_to_idx(2, 2)     # (2,2) -> normal transitions\n",
    "YELLOW_STATE = coord_to_idx(4, 4)  # (4,4) -> normal transitions\n",
    "\n",
    "# Action encoding\n",
    "UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76043dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2898259",
   "metadata": {},
   "source": [
    "### Method 2: Policy Iteration\n",
    "\n",
    "**Policy Evaluation**: Solve $(I - \\gamma P^\\pi)V = R^\\pi$ for current policy $\\pi$\n",
    "\n",
    "**Policy Improvement**: $\\pi'(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V(s')]$\n",
    "\n",
    "**Convergence guarantee**: Each iteration yields $V^{\\pi_{k+1}} \\geq V^{\\pi_k}$ until $V^{\\pi_k} = V^*$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844498b",
   "metadata": {},
   "source": [
    "### Method 3: Value Iteration\n",
    "\n",
    "Direct application of Bellman optimality operator:\n",
    "$$V_{k+1}(s) = \\max_a [R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V_k(s')]$$\n",
    "\n",
    "**Convergence**: $V_k \\to V^*$ as $k \\to \\infty$ by contraction property.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
